{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# <center>Big Data for Engineers \u2013 Exercises</center>\n",
                "## <center>Spring 2020 \u2013 Week 7 \u2013 ETH Zurich</center>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This week we will review _MapReduce_\u2014a programming model and distributed system for processing datasets in parallel over large clusters. We will first discuss the two APIs that can be used to write MapReduce jobs. Then, we willimplement a MapReduce job in Python. Finally, we will discuss relevant theory bits behind MapReduce."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 0. The two APIs for MapReduce\n",
                "\n",
                "MapReduce provides two different interfaces: the **native MapReduce API** and the **Streaming API**.\n",
                "\n",
                "The native MapReduce API is equivalent to the one seen in class. To use it, the user has to write two Java classes: one for the Mapper and one for the Reducer. Just like in the logical model:\n",
                "\n",
                "- the Mapper takes a KeyValue pair and emits zero or more KeyValue pairs;\n",
                "```js\n",
                "function map(key, value)\n",
                "  // Do some work\n",
                "  emit(someKey, someValue)\n",
                "```\n",
                "- and the Reducer takes a key and a collection of values, and emits zero or more KeyValue pairs (usually one). \n",
                "```js\n",
                "function reduce(key, values[])\n",
                "  // Do some work\n",
                "  emit(key, aggregatedValue)\n",
                "```\n",
                "  \n",
                "The Streaming API is usually slower, but allows users to write the Mapper and the Reducer in any language \u2014 even different languages.\n",
                "To use the API we need to write two programs, a mapper and a reducer. In this case, however, the programs will directly read the KV pairs as a sequence of lines from standard input and write the resulting KV pairs to standard output.\n",
                "The streaming API will take care of all the parallelization, the shuffling and everything else required.\n",
                "Since the operations are done using the standard input and standard output, there are two differences with the logical model:\n",
                "\n",
                "1. The KV pairs are not independently processed, but read all **sequentially** from standard input.\n",
                "2. The reducer does not receive a key with a list of values, but the **ordered list** of KV pairs (one per line). Therefore, the reducer must therefore implement itself the logic for handling a new key. "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Hands on\n",
                "\n",
                "In this first part of the exercise session, we will obtain some practical experience with MapReduce. To do so, we will create a cluster on Azure as usual, then write a MapReduce job in Python and use the Hadoop Streaming API to run the code on the cluster."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1.1 Create the cluster\n",
                "\n",
                "By now you are an expert with Azure! Today, we are going to create an HDInsight cluster running Hadoop.\n",
                "\n",
                "**Important:** we want you to use a small but real cluster for running HBase rather than a single machine. But, these clusters burn Azure credit very quickly\u2014the cheapest configuration consumes roughly **2 CHF per hour**, which is a lot relative to your overall credit\u2014so it is very important for you to **delete your cluster once you are done.** Luckily, it is possible to keep your data intact when you delete a cluster, and see it again when you recreate it; we will touch upon this in the process. Now, let's start. Those steps are very similar to the HDFS cluster we create on week 3.\n",
                "\n",
                "1. In Azure portal click the **\"Create a resource\"** button on the left, type **\"hdinsight\"** in the search box, and select **\"Azure HDInsight\"** and click **\"Create\"**. HDInsight is Microsoft's cloud service which wraps Hadoop, HBase, Spark and other Big Data technologies; read more [here](https://azure.microsoft.com/en-us/services/hdinsight/).\n",
                "![marketplace](https://bigdataforeng2020.blob.core.windows.net/exercise07/marketplace.png)  \n",
                "\n",
                "1. In the **\"Basics\"** tab, choose a subscription and **create a new resource group** (say \"exercise7\"). Pick a name for your cluster,\u00a0select **(US) East US** as the region, **Hadoop** as the cluster type. Then, setup the cluster login username and password, as well as the SSH username.\n",
                "![basics](https://bigdataforeng2020.blob.core.windows.net/exercise07/basics.png)  \n",
                "\n",
                "1. Next, we need to configure the **\"Storage\"** tab. The canonical way would be to use an HDFS cluster as a storage layer for a Hadoop cluster, but we will be using the Blob service of Windows Azure Storage for this purpose. This has a significant advantage of allowing you to delete your HBase cluster without losing the data: you can recreate the cluster using the same Azure Storage Account and the same container and you will see the same data. This is useful, for example, if you don't have time to finish this exercise in one sitting: you can just delete your cluster, recreate it later, and continue your work. Azure storage is selected by default (see the screenshot).  \n",
                "To setup your Hadoop cluster for the first time, in **\"Primary storage account\"** click **\"Create new\"** and specify a name. Leave everything else as it is and click \"Next\".\n",
                "**Important**: if you are recreating your Hadoop cluster and want to see the existing data, then choose **\"Select existing\"** and set the container name to the one that you used last time\u2014by default Azure generates a new container name every time you create a cluster, which then points to a different container. The container name can be found in \"Storage account - containers\".\n",
                "![storage](https://bigdataforeng2020.blob.core.windows.net/exercise07/storage.png)  \n",
                "\n",
                "1. In the \"Security + networking\" tab do not choose anything, just click \"Next: Configuration + pricing\".\n",
                "\n",
                "1. Now we need to choose the configuration of the nodes in our HBase cluster. Let's choose **2 Head nodes** of size **D3 v2**, and **4 Worker nodes** of size **D3 v2** (see the screenshot). Click **\"Review + create\"**.\n",
                "![pricing](https://bigdataforeng2020.blob.core.windows.net/exercise07/pricing.png)  \n",
                "\n",
                "1. In the last step, \"Summary\", check if the settings are as you intend. These clusters are expensive, so it is worth checking the price estimate at this step: for me it is 1.74 CHF/hour; if your price is larger than this, check your node sizes and counts. When done, initiate the cluster creation by clicking \"Create\". The process will take time, around 15\u201425 minutes."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1.2 Accessing your cluster\n",
                "To connect to the cluster we run the `ssh` program in a terminal. This process is the same as in last week's HDFS exercise, but we will repeat the instructions here for convenience.\n",
                "\n",
                "There are three options as to how you can do this:\n",
                "1. **On your own machine** you can just use a normal terminal if you have `ssh` installed. Linux usually has it, as does MacOS. Windows doesn't have it by default (although Windows 10 does since recently), but you can use one of the browser-based options described next, or the other option is to install [PuTTY](http://www.putty.org/).\n",
                "1. **In your browser:**\n",
                "  1. Use the **Azure Cloud Shell**. Click on the Cloud Shell icon at the top of Azure Dashboard toolbar. It will request your approval for creating a Storage Account required for the shell; agree to it.\n",
                "  1. Use a **terminal on Jupyter**. In your [notebooks.azure.com](https://notebooks.azure.com) tab, click \"My Projects\" in the upper-left corner of the page. Then, select any project and click \"Terminal\".\n",
                "\n",
                "In your terminal of choice, run the following (this command with everything filled-in is also available on the Azure page of your cluster, if you click \"Secure Shell (SSH)\"): \n",
                "\n",
                "`ssh ssh_user_name@cluster_name-ssh.azurehdinsight.net`\n",
                "\n",
                "In this command, `ssh_user_name` is the \"ssh username\" that you have chosen in the first step of creating the cluster, and `cluster_name` also comes from that form. Note that the cluster name has to be suffixed with `-ssh`. \n",
                "\n",
                "If after running the `ssh` command you see a message similar to this:\n",
                "```\n",
                "Welcome to Hadoop on HDInsight.\n",
                "\n",
                "Last login: Sat Oct 14 15:56:56 2017 from 180.220.17.157\n",
                "To run a command as administrator (user \"root\"), use \"sudo <command>\".\n",
                "See \"man sudo_root\" for details.\n",
                "\n",
                "&lt;ssh_user_name&gt;@hn0-cluster:~$\n",
                "```\n",
                "then you have successfully connected to your cluster.\n",
                "\n",
                "### Troubleshooting\n",
                "Some issues may arise while creating your HBase cluster. Here are some common issues that we experienced:\n",
                "1. *StorageAccountAlreadyExists* : Make sure to use a unique name while creating a new storage account. The portal does not check for this while in the creation panel but only on validation and an error will arise. This also holds for cluster names.\n",
                "1. *The ssh connection does not work* : Use the password that you provided at creation. If you can't retrieve it, you can reset the password in the ssh+keys panel of your Hbase cluster. Also if you are recreating a new cluster, use a different name as your past created cluster. Otherwise, this may create a conflict in your local *known_hosts* configuration file.\n",
                "\n",
                "You can find more information about deployement errors on [this page](https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-manager-common-deployment-errors).&lt;/ssh_user_name&gt;&lt;/cluster_name&gt;&lt;/ssh_user_name&gt;&lt;/cluster_name&gt;&lt;/ssh_user_name&gt;"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1.3 Writing the mapper and the reducer\n",
                "\n",
                "To run a MapReduce job we need of course a mapper function and a reducer. Usually to run natively on Hadoop we need to write our mapper and reducer as classes in Java, but to make the development easier and less cumbersome we are going to use the **Hadoop streaming API**.  \n",
                "This wonderful API allows the developers to write code in any language and integrate it seamlessly with the MapReduce framework.  \n",
                "We just need to provide 2 scripts\u2014one for the mapper, one for the reducer\u2014and let them read the KeyValues from `stdin` (the default input stream) and write them to `stdout` (the default output stream). Hadoop will take care of parallelization, the sort step and everything else required.  \n",
                "\n",
                "To start we will just use the HelloWorld for MapReduce programs, which is WordCount: given a list of files, return for each word the total number of occurrences.  \n",
                "\n",
                "From the ssh console run:\n",
                "\n",
                "`wget https://bigdataforeng2020.blob.core.windows.net/exercise07/mapper.py`  \n",
                "`wget https://bigdataforeng2020.blob.core.windows.net/exercise07/reducer.py`\n",
                "\n",
                "to get the file on the cluster, if you want to edit them or create a new file you can use the simple console text editor `nano` (its commands are indicated at the bottom of its interface, e.g. `^X Exit` means `use Ctrl+X to exit`).\n",
                "\n",
                "Before continuing we need to ensure that the files are actually executable so they can be run by the MapReduce job  \n",
                "`chmod +x ./reducer.py ./mapper.py`.\n",
                "\n",
                "Let's take a closer look at the files:\n",
                "\n",
                "```python\n",
                "#!/usr/bin/env python\n",
                "\"\"\"mapper.py\"\"\"\n",
                "\n",
                "import sys\n",
                "\n",
                "for line in sys.stdin:\n",
                "    line = line.strip()\n",
                "    words = line.split()\n",
                "    for word in words:\n",
                "        print '%s\\t%s' % (word, 1)\n",
                "```\n",
                "For the mapper the first line starting with `#!` tells to Hadoop how to run the script (using Python in this case), then for each line in our input (automatically directed to the `sys.stdin` stream by Hadoop) we remove leading and trailing whitespaces, then split the line on each whitespace and 'emit' a key-value pair made up of a word and the unit count one.\n",
                "\n",
                "```python\n",
                "#!/usr/bin/env python\n",
                "\"\"\"reducer.py\"\"\"\n",
                "\n",
                "from operator import itemgetter\n",
                "import sys\n",
                "\n",
                "current_word = None\n",
                "current_count = 0\n",
                "word = None\n",
                "\n",
                "for line in sys.stdin:\n",
                "    line = line.strip()\n",
                "    word, count = line.split('\\t', 1)\n",
                "    try:\n",
                "        count = int(count)\n",
                "    except ValueError:\n",
                "        continue\n",
                "\n",
                "    if current_word == word:\n",
                "        current_count += count\n",
                "    else:\n",
                "        if current_word:\n",
                "            print '%s\\t%s' % (current_word, current_count)\n",
                "        current_count = count\n",
                "        current_word = word\n",
                "\n",
                "if current_word == word:\n",
                "    print '%s\\t%s' % (current_word, current_count)\n",
                "```\n",
                "For the reducer we receive an ordered list of key-value pairs generated by the mapper and then sorted automatically, so for each line in the input stream, we remove leading and trailing whitespaces, we split the line into the word and the count (always 1 if we used the previous mapper and no combiner), then try to convert the count (by default a string) to a number (and skipping the value in case of failure).  \n",
                "After that if the word is equal to the previous one (remember the kv pairs are sorted so all the same words will be together) we just increase the count for that word by one, otherwise we print the current word with the associated count and move to the next word."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1.4 Test correctness of your program locally\n",
                "\n",
                "Since a MapReduce job on a cluster usually requires a considerable amount of time, before launching it we want to test our functions locally.  \n",
                "To do so we can simulate our MapReduce job by inputting the data to the mapper, properly sorting the output of that and feeding it into the reducer, then checking that we get the expected result.  \n",
                "\n",
                "We can try with   `echo \"foo foo quux labs foo bar quux\" | ./mapper.py | sort -k1,1 | ./reducer.py`."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1.5 Get some data\n",
                "\n",
                "Download on the cluster some nice books on which we will run our MapReduce job (btw some of these are really nice)\n",
                "```\n",
                "wget http://www.gutenberg.org/cache/epub/2500/pg2500.txt\n",
                "wget http://www.gutenberg.org/files/1342/1342-0.txt\n",
                "wget http://www.gutenberg.org/files/84/84-0.txt\n",
                "wget http://www.gutenberg.org/files/2600/2600-0.txt\n",
                "wget http://www.gutenberg.org/files/74/74-0.txt\n",
                "wget http://www.gutenberg.org/files/2591/2591-0.txt\n",
                "wget http://www.gutenberg.org/files/4300/4300-0.txt\n",
                "```\n",
                "\n",
                "and put them on HDFS\n",
                "```\n",
                "hadoop fs -mkdir /tmp/books\n",
                "hadoop fs -copyFromLocal ./*.txt /tmp/books\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1.6 Run the MapReduce job\n",
                "Finally we are ready to run our MapReduce job:  \n",
                "```\n",
                "hadoop jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
                "-file /home/sshuser/mapper.py    -mapper /home/sshuser/mapper.py \\\n",
                "-file /home/sshuser/reducer.py   -reducer /home/sshuser/reducer.py \\\n",
                "-input /tmp/books/* -output /tmp/output-folder\n",
                "```\n",
                "\n",
                "This command allows us to use the streaming API from Hadoop. We need to pass each file used, the mapper and the reducer and finally the input files and the output folder (__the output folder must be a new non-already-existing folder__).  \n",
                "We can pass additional configuration parameters (eg. we can ask Hadoop to use a certain number of reducers) by using the D argument.  \n",
                "```\n",
                "hadoop jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
                "-D mapred.reduce.tasks=4 \\\n",
                "-file /home/sshuser/mapper.py    -mapper /home/sshuser/mapper.py \\\n",
                "-file /home/sshuser/reducer.py   -reducer /home/sshuser/reducer.py \\\n",
                "-input /tmp/books/* -output /tmp/output-folder\n",
                "```\n",
                "\n",
                "We can check the result of our job by inspecting the output folder:  \n",
                "`hadoop fs -ls /tmp/output-folder`  \n",
                "The output is found under files call `part-xxxxx`. Usually one file per reducer is generated unless the input is small. We can examine our file by using:  \n",
                "`hadoop fs -cat /tmp/output-folder/part-00000`"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1.7 Visualizing the MapReduce job\n",
                "\n",
                "Now that we have launched our first job we can explore on the __Ambari__ dashboard the CPU and memory consumption and the details of our job.  \n",
                "Access the Ambari Dashboard by clicking __Ambari home__ (see image) from your cluster overview page (on the azure dashboard __all resources__ in the left menu, select your cluster in the list, it is an __HDInsight__ resource).\n",
                "![access_ambari](https://bigdataforeng2020.blob.core.windows.net/exercise07/access_ambari.png)  \n",
                "Then from the Ambari homepage you can use the __Mapreduce2__ service to see the job history or the __YARN__ service to see the current resource usage and monitor current jobs.  \n",
                "![ambari_home](https://bigdataforeng2020.blob.core.windows.net/exercise07/ambari_home.png)  \n",
                "Go to MapReduce service and access the job history under \"Quick Links\" &gt; first link &gt; \"JobHistory UI\".\n",
                "![mapreduce_home](https://bigdataforeng2020.blob.core.windows.net/exercise07/mr-home.png)  \n",
                "From this interface you can explore the executed jobs, the result of the jobs, the number of mappers and reducers used and a lot of other useful information.\n",
                "![mapreduce](https://bigdataforeng2020.blob.core.windows.net/exercise07/mr.png)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now we will try to launch our job again and monitor it live with the YARN service. First from the Ambari interface access the __YARN__ service and then again from \"Quick Links\" &gt; choose the \"active\" node &gt; \"ResourceManager UI\".\n",
                "![yarn_home](https://bigdataforeng2020.blob.core.windows.net/exercise07/yarn-home.png)  \n",
                "![yarn](https://bigdataforeng2020.blob.core.windows.net/exercise07/yarn.png) \n",
                "YARN is a complex framework that handles resource management on the cluster. You will get to know more about it during the YARN lecture, but for now we will just use it to monitor our MapReduce job live.  \n",
                "Now from the ssh shell run again the same MapReduce job, but remember to change the output folder to a new name, otherwise the job will fail because the folder already exists.  \n",
                "```\n",
                "hadoop jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
                "-D mapred.reduce.tasks=4 \\\n",
                "-file /home/sshuser/mapper.py    -mapper /home/sshuser/mapper.py \\\n",
                "-file /home/sshuser/reducer.py   -reducer /home/sshuser/reducer.py \\\n",
                "-input /tmp/books/* -output /tmp/output-folder2\n",
                "```\n",
                "\n",
                "If you now go to your dashboard and refresh the browser, you will see a new running job and by clicking on it you can examine step by step the resources used by the various tasks and the progress of our job. This is particularly useful to monitor and debug our application and examine for example the performance of the different nodes on our cluster.\n",
                "\n",
                "This was all for the hands-on part! Exit the shell and finally"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### <span style=\"color: #C00000;\">1.8 DELETE THE CLUSTER</span>\n",
                "As always, this cluster burns a lot of money so delete it now!  \n",
                "From the dashboard click in the left menu 'All resources', check the 'HDInsight' cluster resource in the list and click 'delete' in the top menu, confirm that you want to delete the resource and **DELETE** it!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Understanding MapReduce's execution model\n",
                "\n",
                "For each of the following statements, state whether it is *true* or *false* and briefly explain why.\n",
                "\n",
                "1. Each mapper must generate the same number of key/value pairs as its input had.\n",
                "2. The TaskTracker is responsible for scheduling mappers and reducers and make sure all nodes are correctly running.\n",
                "3. Mappers input key/value pairs are sorted by the key.\n",
                "4. MapReduce splits might not correspond to HDFS block.\n",
                "5. One single Reducer is applied to all values associated with the same key.\n",
                "6. Multiple Reducers can be assigned pairs with the same value.\n",
                "7. In Hadoop MapReduce, the key-value pairs a Reducer outputs must be of the same type as its input pairs."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. A comprehension task\n",
                "Conceptually, a map function takes in input a kay-value pair and emits a list of key-values pairs, while a reduce function takes in input a key with an associated list of values and returns a list of values or key-value pairs. Often the type of the final key and value is the same of the type of the intermediate data:\n",
                "\n",
                "- map     `(k1,v1) --&gt; list(k2,v2)`\n",
                "- reduce  `(k2,list(v2))--&gt; list(k2, v2)`\n",
                "\n",
                "Analyze the following Mapper and Reducer, written in pseudo-code, and answer the questions below.\n",
                "\n",
                "```js\n",
                "function map(key, value)\n",
                "  emit(key, value);\n",
                "```\n",
                "\n",
                "```js\n",
                "function reduce(key, values[])\n",
                "  z = 0.0\n",
                "  for value in values:\n",
                "    z += value\n",
                "  emit(key, z / values.length())\n",
                "```\n",
                "\n",
                "** Questions **\n",
                "1. Explain what is the result of running this job on a list of pairs with type ([string], [float]).\n",
                "2. Could you use this reduce function as combiner as well? Why or why not?\n",
                "3. If your answer to the previous question was *yes*, does the number of different keys influences the effectiveness of the combiner? If you answer was *no*, can you change (if needed) map and reduce functions in such a way the new reducer the can be used as combiner?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Own Exploration\n",
                "\n",
                "Imagine you are given a dataset with the temperatures and precipitations around the world for a given day.  \n",
                "The initial KV pairs consists of `(line number in the file) -&gt; (country,station_id,avg_temperature,mm_of_rain)`.  \n",
                "You can assume that all station IDs are distinct.   \n",
                "Write a MapReduce job (using pseudocode as seen in task 3) for each of the following problems, also state whether it is possible to use a combiner to speed up the computation.\n",
                "\n",
                "1. Find for each country except the UK the maximum avg_temperature  \n",
                "2. Find for each country the station_id with the maximum avg_temperature  \n",
                "3. Find for each country the total amount of mm_of_rain but only for countries for which the total is greater than 100mm  \n",
                "4. Find for each country the total amount of mm_of_rain from stations in which it rained more than 10mm  "
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.5.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
